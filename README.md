# FairMark V2.0 - Automated Education Evaluation System

## ğŸ¯ Overview

FairMark is an **always-running, event-driven evaluation system** that monitors ALL Canvas LMS submissions 24/7 and provides automated AI-powered feedback.

### âœ¨ Key Features

- âœ… **Always Running** - Monitors all courses and assignments automatically
- âœ… **Dynamic Discovery** - No manual configuration needed
- âœ… **Instant Detection** - Checks every 30 seconds for new submissions
- âœ… **Multiple Attempts** - Each resubmission gets separate evaluation
- âœ… **AI-Powered** - GPT-4 based structured feedback
- âœ… **Canvas Integration** - Automatic comment posting
- âœ… **Testing API** - Comprehensive endpoints for testing

## ğŸš€ Quick Start

### Start the System

```bash
cd /Users/utin/Desktop/Desktop/FairMark
./start_watcher.sh
```

**That's it!** The system will:
1. Start the FastAPI server on port 8000
2. Launch the watcher service automatically
3. Begin monitoring ALL courses and assignments
4. Detect and evaluate new submissions instantly

### Stop the System

Press `Ctrl+C` in the terminal

## ğŸ“Š API Endpoints

Once running, access these endpoints:

### Documentation
- **Interactive API Docs**: http://127.0.0.1:8000/docs
- **Health Check**: http://127.0.0.1:8000/health

### Status & Monitoring
- **Watcher Status**: http://127.0.0.1:8000/watcher/status
- **Tracked Submissions**: http://127.0.0.1:8000/watcher/submissions

### Testing Endpoints
- **List Courses**: http://127.0.0.1:8000/test/courses
- **List Assignments**: http://127.0.0.1:8000/test/assignments/{course_id}
- **List Submissions**: http://127.0.0.1:8000/test/submissions/{course_id}/{assignment_id}
- **Mock Evaluation**: POST to http://127.0.0.1:8000/test/evaluate-mock

## ğŸ§ª Testing Your System

### 1. Check Health
```bash
curl http://127.0.0.1:8000/health
```

### 2. List Your Courses
```bash
curl http://127.0.0.1:8000/test/courses
```

### 3. List Assignments
```bash
curl http://127.0.0.1:8000/test/assignments/YOUR_COURSE_ID
```

### 4. Test Mock Evaluation
```bash
curl -X POST "http://127.0.0.1:8000/test/evaluate-mock?course_id=COURSE_ID&assignment_id=ASSIGNMENT_ID&user_id=USER_ID&submission_id=SUBMISSION_ID"
```

Check Canvas to see if the test comment was posted!

## ğŸ“‹ How It Works

```
System Startup
    â†“
FastAPI Server Starts (Port 8000)
    â†“
Watcher Auto-Starts (Background Thread)
    â†“
Every 30 seconds:
  â†’ Scan all active courses
  â†’ Check all assignments
  â†’ Look for new submissions
    â†“
New Submission Detected
    â†“
  â†’ Extract metadata (student_id, submission_id, attempt, timestamp)
  â†’ Download submission file
  â†’ AI evaluation (GPT-4)
  â†’ Generate structured feedback
  â†’ Post comment to Canvas: [Attempt #N] + feedback
  â†’ Mark as processed
    â†“
Continue monitoring...
```

## ğŸ“ Configuration

### Required Environment Variables

Create a `.env` file with:

```bash
CANVAS_BASE_URL=https://canvas.instructure.com
CANVAS_TOKEN=your_canvas_api_token_here
OPENAI_API_KEY=your_openai_api_key_here
POLICY_TEXT=Your evaluation policy text here
```

### Optional Configuration

- **Check Interval**: Edit `app/watcher.py`, change `SubmissionWatcher(check_interval=30)`
- **Port**: Edit `start_watcher.sh`, change `--port 8000`

## ğŸ“‚ Project Structure

```
FairMark/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application with all endpoints
â”‚   â”œâ”€â”€ watcher.py           # Continuous watcher service
â”‚   â”œâ”€â”€ canvas_client.py     # Canvas API client
â”‚   â”œâ”€â”€ llm_client.py        # OpenAI integration
â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”œâ”€â”€ resolver.py          # Submission resolver
â”‚   â”œâ”€â”€ policy.py            # Policy and late submission logic
â”‚   â”œâ”€â”€ prompt_builder.py    # Evaluation prompt builder
â”‚   â”œâ”€â”€ file_parser.py       # File parsing utilities
â”‚   â”œâ”€â”€ file_utils.py        # File download utilities
â”‚   â””â”€â”€ config.py            # Configuration
â”œâ”€â”€ start_watcher.sh         # Startup script
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env                     # Environment variables (not in git)
â”œâ”€â”€ README_V2.md            # Complete documentation
â”œâ”€â”€ QUICKSTART_V2.md        # Quick start guide
â””â”€â”€ COMMANDS.md             # Command reference
```

## ğŸ“ Canvas Comment Format

Each evaluation posts a comment with:

```
[Attempt #N]

Overall evaluation (short):
Brief summary of the submission...

Rubric breakdown:
Criterion 1 â€” X/Y points
Comment: Detailed feedback...

Criterion 2 â€” X/Y points
Comment: Detailed feedback...

Possible Final Grade: X/Y

Note: [Any additional notes]
```

## ğŸ”§ Requirements

- Python 3.9+
- Canvas API token with submission and comment permissions
- OpenAI API key
- FastAPI and dependencies (see requirements.txt)

## ğŸ“š Documentation

- **README_V2.md** - Complete system documentation
- **QUICKSTART_V2.md** - Quick start guide with examples
- **COMMANDS.md** - All available commands
- **API Docs** - http://127.0.0.1:8000/docs (when running)

## ğŸ¯ Key Advantages

### V2.0 vs V1.0

| Feature | V1.0 | V2.0 |
|---------|------|------|
| Assignment Config | Manual JSON | Fully Automatic |
| Discovery | Static List | Dynamic Scanning |
| Startup | Multiple Scripts | Single Command |
| Watcher | External Script | Built into FastAPI |
| Check Interval | 5 minutes | 30 seconds |
| Multiple Attempts | Basic | Full Tracking |
| Testing | Manual | Built-in API |
| Logging | Mixed | Structured |

## ğŸ› ï¸ Troubleshooting

### Port Already in Use
```bash
# Kill process on port 8000
lsof -ti:8000 | xargs kill -9

# Restart
./start_watcher.sh
```

### Watcher Not Detecting
- Check Canvas API token permissions
- Verify environment variables in `.env`
- Test endpoints in http://127.0.0.1:8000/docs

### Comments Not Posting
- Test with `/test/evaluate-mock` endpoint
- Check Canvas API token has comment permissions
- Verify user_id is correct

## ğŸ“ Support

For issues or questions:
1. Check the logs in your terminal
2. Test endpoints at http://127.0.0.1:8000/docs
3. Review README_V2.md for detailed documentation

## ğŸ“„ License

This project is for educational purposes.

## ğŸ‰ Status

**FULLY OPERATIONAL** - System is production-ready and monitoring submissions 24/7!

---

**Start monitoring now:** `./start_watcher.sh`
# FairMark
